{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019-09-28\n",
    "\n",
    "1. 클래스\n",
    "2. 정규표현식 --> pandas\n",
    "3. HTML 분석\n",
    "4. 크롤링\n",
    "\n",
    "***\n",
    "\n",
    "### - HTML(Hyper Text Markup Language)\n",
    "+ 링크로 연결된 텍스트 페이지  \n",
    "+ 자유롭게 이동이 가능하다.  \n",
    "+ 웹페이지를 구성한다. -> HTML(뼈대), CSS(옷), Javasript(특수기능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1=스크레이핑이란?\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# 분석하고 싶은 HTML 코드\n",
    "html1= '''\n",
    "<html>\n",
    "    <body>\n",
    "        <h1>스크레이핑이란?</h1>\n",
    "        <p>웹 페이지를 분석하는 것</p>\n",
    "        <p>원하는 부분을 추출하는 것</p>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "html2 = 'C://Users/dazzul/Documents/GitHub/python_basic/20190928/bs4.html'\n",
    "\n",
    "# HTML 분석하기\n",
    "soup = BeautifulSoup(open(html2,encoding='utf8'), 'html.parser', )\n",
    "\n",
    "# 원하는 부분 선택하여 출력하기\n",
    "h1 = soup.html.body.h1\n",
    "\n",
    "#선택된요소에서 글자 추출\n",
    "print(\"h1=\"+h1.string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 첫번째 p태그 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1=스크레이핑이란?\n",
      "p1=웹 페이지를 분석하는 것\n",
      "p2=원하는 부분을 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# 분석하고 싶은 HTML 코드\n",
    "html1= '''\n",
    "<html>\n",
    "    <body>\n",
    "        <h1>스크레이핑이란?</h1>\n",
    "        <p>웹 페이지를 분석하는 것</p>\n",
    "        <p>원하는 부분을 추출하는 것</p>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "html2 = 'C://Users/dazzul/Documents/GitHub/python_basic/20190928/bs4.html'\n",
    "\n",
    "# HTML 분석하기\n",
    "soup = BeautifulSoup(open(html2,encoding='utf8'), 'html.parser', )\n",
    "\n",
    "# 원하는 부분 선택하여 출력하기\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "p2 = p1.next_sibling.next_sibling\n",
    "\n",
    "#선택된요소에서 글자 추출\n",
    "print(\"h1=\"+h1.string)\n",
    "print(\"p1=\"+p1.string)\n",
    "print(\"p2=\"+p2.string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### id로 찾기(find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title=스크레이핑이란?\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# 분석하고 싶은 HTML 코드\n",
    "html= '''\n",
    "<html>\n",
    "    <body>\n",
    "        <h1 id=\"title\">스크레이핑이란?</h1>\n",
    "        <p>웹 페이지를 분석하는 것</p>\n",
    "        <p>원하는 부분을 추출하는 것</p>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "# HTML 분석하기\n",
    "soup = BeautifulSoup(html, 'html.parser', )\n",
    "\n",
    "# 원하는 부분 선택하여 출력하기\n",
    "title = soup.find(id=\"title\")\n",
    "\n",
    "#선택된요소에서 글자 추출\n",
    "print(\"title=\"+title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기상청 예보 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기상청 육상 중기예보\n",
      "기압골과 동풍의 영향으로 10월 1일과 2일은 강원영동과 남부지방, 제주도에 비가 오겠고, 강원영동과 경상도는 3일까지 이어지는 곳이 있겠습니다.<br />한편, 동풍의 영향으로 5일 강원영동에는 비가 오겠습니다. 그 밖의 날은 고기압의 가장자리에 들어 구름많겠습니다.<br />기온은 평년(최저기온: 7~17℃, 최고기온: 21~25℃)보다 조금 높겠습니다.<br />강수량은 평년(1~6mm)보다 중부지방(강원영동 제외)은 적겠으나, 강원영동과 남부지방, 제주도는 많겠습니다.<br /><br />* 괌 서쪽해상에 위치한 열대저압부의 발달과 이동경로에 따라 10월 1일 이후의 예보 변동성이 크겠으니, 앞으로 발표되는 예보와 기상정보를 참고하기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = 'http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp'\n",
    "\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "title = soup.find(\"title\").string\n",
    "wf = soup.find(\"wf\").string\n",
    "print(title)\n",
    "print(wf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <li> 리스트 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "좋아하는 영화\n",
      "너무\n",
      "돈을\n",
      "쉽게\n",
      "버시네\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <div id=\"meigen\">\n",
    "        <h1>좋아하는 영화</h1>\n",
    "        <ul class=\"items\">\n",
    "            <li>너무</li>\n",
    "            <li>돈을</li>\n",
    "            <li>쉽게</li>\n",
    "            <li>버시네</li>\n",
    "        </ul>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "h1 = soup.select_one(\"div#meigen>h1\").string\n",
    "print(h1)\n",
    "\n",
    "li_list = soup.select(\"div#meigen>ul.items>li\")\n",
    "for li in li_list:\n",
    "    print(li.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네이버 금융에서 환율 정보 추측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,200.00\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "price = soup.select_one(\"#exchangeList > li.on > a.head.usd > div > span.value\").string\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,200.00\n",
      "1,110.03\n",
      "1,312.44\n",
      "168.51\n",
      "108.1400\n",
      "1.0947\n",
      "1.2313\n",
      "98.7600\n",
      "55.91\n",
      "1542.1\n",
      "1499.1\n",
      "57711.71\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "prices = soup.select(\"span.value\")\n",
    "for price in prices:\n",
    "    print(price.string)\n",
    "\n",
    "# print(\"달러 = \", price[0].string + \"원입니다\")\n",
    "# print(\"엔화 = \", price[1].string + \"원입니다\")\n",
    "# print(\"유로화 = \", price[2].string + \"원입니다\")\n",
    "# print(\"위엔화\", price[3].string + \"원입니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네이버 뉴스 헤드라인 5개 뽑아오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing C:/Users/dazzul/Documents/GitHub/python_basic/20190928/aa.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile C:/Users/dazzul/Documents/GitHub/python_basic/20190928/aa.py\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"https://news.naver.com/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res, 'html.parser')\n",
    "# #today_main_news > div.hdline_news > ul > li:nth-child(1) > div.hdline_article_tit > a\n",
    "a_links = soup.select('#today_main_news > div.hdline_news > ul > li > div.hdline_article_tit > a')\n",
    "\n",
    "for a_link in a_links:\n",
    "    name = a_link.string\n",
    "    print(name.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
